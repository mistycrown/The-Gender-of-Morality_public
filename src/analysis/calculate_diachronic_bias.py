"""
================================================================================
é“å¾·è¯å…¸æ€§åˆ«åè§ MVPè®¡ç®—è„šæœ¬ (å†æ—¶ç‰ˆ - å…¨é‡å®éªŒ)
================================================================================
æ¯æ¬¡æ›´æ–°ä»£ç æ–‡ä»¶æ—¶è¯·æ›´æ–°ä»£ç å¤´æ³¨é‡Šï¼

ã€åŠŸèƒ½æè¿°ã€‘
1. è¯»å– static/dic/ ä¸‹çš„é“å¾·è¯å…¸æ–‡ä»¶ï¼ˆå…¨é‡è¯»å–ï¼‰ã€‚
2. åœ¨å„æœä»£è¯­æ–™åº“ä¸­æŠ½å–å¥å­ï¼š
   - å•ä¹‰è¯ï¼šä»è¯­æ–™åº“éšæœºæŠ½æ ·
   - å¤šä¹‰è¯ï¼šä» data/ngram_match è¯»å–ç­›é€‰åçš„ä¾‹å¥
3. ä½¿ç”¨ SikuBERT ä¸åŒæå–å†æ—¶åŠ¨æ€è¯å‘é‡ã€‚
4. è®¡ç®—æ¯ä¸ªè¯åœ¨å¯¹åº”æœä»£æ€§åˆ«è½´çº¿ä¸Šçš„æŠ•å½±å¾—åˆ†ã€‚
5. è¾“å‡ºå†æ—¶å˜åŒ– JSON æ•°æ®å’ŒæŠ¥å‘Šã€‚

ã€è¾“å…¥ã€‘
- è¯å…¸: static/dic/*.txt
- å¤šä¹‰è¯ä¾‹å¥: data/ngram_match/*.txt
- æ€§åˆ«è½´çº¿: result/20251230-diachronic-axis/dynasty_gender_axes.npy
- è¯­æ–™åº“: data/æ ¸å¿ƒå¤ç±/æ ¸å¿ƒå¤ç±-*.txt

ã€è¾“å‡ºã€‘
- result/20251230-moral-mvp-diachronic/moral_bias_mvp_diachronic_full.json
- result/20251230-moral-mvp-diachronic/moral_bias_mvp_diachronic_full_report.md

ã€å‚æ•°è¯´æ˜ã€‘
- --dic-dir: è¯å…¸ç›®å½•
- --data-dir: ngram_match ç›®å½•
- --corpus-dir: æ ¸å¿ƒå¤ç±è¯­æ–™ç›®å½•
- --axis-file: å†æ—¶æ€§åˆ«è½´çº¿æ–‡ä»¶
- --output-dir: ç»“æœè¾“å‡ºç›®å½•
- --sample-size: æ¯ä¸ªè¯æ¯æœä»£æŠ½æ ·å¥å­æ•° (é»˜è®¤ 50)

ã€ä¾èµ–åº“ã€‘
- transformers, torch, numpy, tqdm

ã€ä½œè€…ã€‘AIè¾…åŠ©ç”Ÿæˆ
ã€åˆ›å»ºæ—¥æœŸã€‘2026-01-11
ã€æœ€åä¿®æ”¹ã€‘- 2024-12-30: åˆ›å»ºåˆå§‹ç‰ˆæœ¬ï¼Œæ”¯æŒç®€å•çš„åŒä¹‰ç­›é€‰
- 2025-01-11: ä¼˜åŒ– & è¿ç§» AutoDL
    1. å¢åŠ  Batch Processing (128) ä¼˜åŒ– 3090 æ€§èƒ½
    2. å¢åŠ  AutoDL ç¯å¢ƒè‡ªåŠ¨è¯†åˆ« (/root/autodl-tmp)
    3. å¢åŠ æœ¬åœ°æ¨¡å‹è‡ªåŠ¨æ£€æµ‹ (ä¼˜å…ˆäº HF ä¸‹è½½)
    4. ä¿®å¤è¯å…¸è¯»å–é€»è¾‘ï¼ˆå¿½ç•¥åˆ†éš”çº¿åå†…å®¹ï¼‰
    5. å¢åŠ å¯åŠ¨æç¤ºï¼Œé¿å…åŠ è½½æ—¶å‡æ­»ä½“éªŒ
    6. ä¿®å¤ IndentationError å’Œå˜é‡åˆå§‹åŒ–é—®é¢˜

ã€ä¿®æ”¹å†å²ã€‘
- 2026-01-11: åŸºäº calculate_moral_mvp_diachronic.py åˆ›å»ºå…¨é‡ç‰ˆï¼Œå¢åŠ  Colab æ”¯æŒ
"""

import os

# å¿…é¡»åœ¨ import transformers ä¹‹å‰è®¾ç½®ï¼Œå¦åˆ™å¯èƒ½æ— æ•ˆ
if os.path.exists("/root/autodl-tmp"):
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

print("â³ æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒ (åŠ è½½ PyTorch/Transformers)...") # æç¤ºç”¨æˆ·æ­£åœ¨åŠ è½½åº“

import sys
import glob
import re
import argparse
import random
import json
import numpy as np
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModel
from typing import List, Dict, Tuple, Optional
from tqdm import tqdm

# ==================== é»˜è®¤é…ç½® ====================
DEFAULT_MODEL_NAME = "SIKU-BERT/sikubert"
DEFAULT_DIC_DIR = "static/dic"
DEFAULT_NGRAM_MATCH_DIR = r"data\ngram_match_sampled"
DEFAULT_CORPUS_DIR = r"data\æ ¸å¿ƒå¤ç±"
DEFAULT_AXIS_FILE = r"result\20251230-diachronic-axis\dynasty_gender_axes.npy"
DEFAULT_OUTPUT_DIR = r"result\20251230-moral-mvp-diachronic"
DEFAULT_SAMPLE_SIZE = 200
BATCH_SIZE = 128  # é’ˆå¯¹ 3090 ä¼˜åŒ–
POLYSEMY_THRESHOLD = 0.05  # å¤šä¹‰è¯ç›¸ä¼¼åº¦æˆªæ–­é˜ˆå€¼

# colab é»˜è®¤è·¯å¾„
COLAB_ROOT = Path("/content/drive/MyDrive/moral_bias_data")
COLAB_DIC_DIR = COLAB_ROOT / "dic"
COLAB_NGRAM_MATCH_DIR = COLAB_ROOT / "ngram_match"
COLAB_CORPUS_DIR = COLAB_ROOT / "corpus"
COLAB_AXIS_FILE = COLAB_ROOT / "gender_axis/dynasty_gender_axes.npy"
COLAB_OUTPUT_DIR = COLAB_ROOT / "result/20251230-moral-mvp-diachronic"

# æœä»£æ˜ å°„
DYNASTY_MAP = {
    'å…ˆç§¦ä¸¤æ±‰': 'Aå…ˆç§¦ä¸¤æ±‰', 'é­æ™‹å—åŒ—æœ': 'Bé­æ™‹å—åŒ—æœ', 'éš‹å”': 'Céš‹å”',
    'å®‹': 'Då®‹', 'å…ƒ': 'Eå…ƒ', 'æ˜': 'Fæ˜', 'æ¸…': 'Gæ¸…'
}
DYNASTY_ORDER = ['å…ˆç§¦ä¸¤æ±‰', 'é­æ™‹å—åŒ—æœ', 'éš‹å”', 'å®‹', 'å…ƒ', 'æ˜', 'æ¸…']

# è¯å…¸æ–‡ä»¶ååˆ°ç±»åˆ«çš„æ˜ å°„
CAT_MAP = {
    "01care.txt": "Care", "02aut.txt": "Authority", "03loy.txt": "Loyalty",
    "04fair.txt": "Fairness", "05san.txt": "Sanctity", "06lenience.txt": "Lenience",
    "07waste.txt": "Waste", "08altruism.txt": "Altruism", "09diligence.txt": "Diligence",
    "10resilience.txt": "Resilience", "11modesty.txt": "Modesty", "12valor.txt": "Valor"
}

def is_polysemous_word(word: str, dict_filepath: str) -> bool:
    """æ£€æŸ¥è¯æ±‡æ˜¯å¦ä¸ºå¤šä¹‰è¯"""
    try:
        with open(dict_filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('---'): continue
                
                if 'å¤šä¹‰' in line:
                    parts = line.split(',')
                    if len(parts) >= 1 and parts[0].strip() == word:
                        return True
    except Exception:
        pass
    return False

def load_filtered_sentences_for_polysemy(word, category_code, label, data_dir, dynasty_code=None, limit=200):
    """è¯»å–å¤šä¹‰è¯ç­›é€‰ä¾‹å¥ï¼Œè¿”å› (æœä»£, ä¾‹å¥æ­£æ–‡) åˆ—è¡¨"""
    # å°è¯•å¤šä¸ªæ ‡ç­¾å˜ä½“
    label_variants = [label]
    if label == 'neg':
        label_variants.extend(['nag', 'negative'])
    elif label == 'pos':
        label_variants.extend(['positive'])
    
    matches = []
    for label_variant in label_variants:
        pattern = f"{category_code}-{label_variant}-{word}-*.txt"
        matches = glob.glob(os.path.join(data_dir, pattern))
        if matches:
            break
    
    if not matches: return []
    
    filepath = matches[0]
    sentences = []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line: continue
                
                # æå–æ‰€æœ‰æ–¹æ‹¬å·å†…å®¹
                brackets = re.findall(r'\[([^\]]+)\]', line)
                if len(brackets) < 3:
                    continue
                
                # ç¬¬ä¸€ä¸ªæ–¹æ‹¬å·ï¼šæœä»£æ ‡è®° (å¦‚ "Då®‹")
                dynasty_tag = brackets[0]
                
                # æœä»£ç­›é€‰
                if dynasty_code and not dynasty_tag.startswith(dynasty_code):
                    continue
                
                # ç¬¬ä¸‰ä¸ªæ–¹æ‹¬å·ï¼šç›¸ä¼¼åº¦
                try:
                    similarity = float(brackets[2])
                except:
                    similarity = 0.0
                
                # æå–ä¾‹å¥æ­£æ–‡ï¼šå»é™¤å‰ä¸‰ä¸ªæ–¹æ‹¬å·åŠå…¶å†…å®¹
                content = re.sub(r'^(\[[^\]]+\]\s*){3}', '', line)
                content = content.replace('âœ“', '').replace('âœ—', '').strip()
                
                is_pos = 'âœ“' in line
                is_neg = 'âœ—' in line
                
                if is_neg: continue
                if not is_pos and similarity <= POLYSEMY_THRESHOLD: continue
                
                if 5 <= len(content) <= 500:
                    # æå–æœä»£åç§°ï¼ˆå»æ‰å‰ç¼€å­—æ¯ï¼‰
                    dynasty = dynasty_tag[1:] if len(dynasty_tag) > 1 else dynasty_tag
                    sentences.append((dynasty, content))
    except: pass
    
    if len(sentences) > limit:
        sentences = random.sample(sentences, limit)
    return sentences

def extract_sentences_from_corpus(corpus_dir, regex_pattern, target_words, limit_per_word):
    """ä»è¯­æ–™åº“ä¸­æå–ä¾‹å¥ï¼Œè¿”å› {è¯æ±‡: [(æœä»£, ä¾‹å¥), ...]} æ ¼å¼
    
    Args:
        regex_pattern: æœä»£ä»£ç ï¼Œç”¨äºåŒ¹é…æ–‡ä»¶åï¼ˆå¦‚ "Då®‹"ï¼‰
    """
    word_sents = {w: [] for w in target_words}
    files = glob.glob(os.path.join(corpus_dir, f"*{regex_pattern}*.txt"))
    random.shuffle(files)
    
    print(f"  è¯­æ–™æ–‡ä»¶: {len(files)} (æ¨¡å¼: {regex_pattern})")
    
    # ä»regex_patternæå–æœä»£åç§°ï¼ˆå¦‚ "Då®‹" -> "å®‹"ï¼‰
    dynasty_name = regex_pattern[1:] if len(regex_pattern) > 1 and regex_pattern[0].isalpha() else regex_pattern
    
    # å¢åŠ æ–‡ä»¶çº§è¿›åº¦æ¡
    pbar = tqdm(files, desc="  æ‰«æè¯­æ–™", unit="file")
    for fp in pbar:
        try:
            with open(fp, 'r', encoding='utf-8') as f:
                content = f.read()
                sents = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
                for s in sents:
                    s = s.strip()
                    if len(s) < 5 or len(s) > 500: continue
                    for w in target_words:
                        if len(word_sents[w]) >= limit_per_word: continue
                        if w in s:
                            word_sents[w].append((dynasty_name, s))  # è¿”å›å…ƒç»„æ ¼å¼
        except Exception as e:
            print(f"  âŒ Error reading {os.path.basename(fp)}: {e}")
    
    # ç»Ÿè®¡ç»“æœ
    missing = [w for w, sents in word_sents.items() if len(sents) == 0]
    if missing:
        print(f"  âš ï¸  æœªæ‰¾åˆ°ä¾‹å¥çš„è¯: {missing[:3]}{'...' if len(missing) > 3 else ''}")
    
    return word_sents

def get_contextual_vector(model, tokenizer, device, sentences: List[Tuple[str, str]], target_word: str) -> Optional[np.ndarray]:
    """è·å–ä¸Šä¸‹æ–‡å‘é‡
    
    Args:
        sentences: [(æœä»£, ä¾‹å¥æ­£æ–‡), ...] å…ƒç»„åˆ—è¡¨
        target_word: ç›®æ ‡è¯æ±‡
    """
    vectors = []
    # æå–ä¾‹å¥æ­£æ–‡éƒ¨åˆ†
    target_sentences = [sent for _, sent in sentences]
    
    for i in range(0, len(target_sentences), BATCH_SIZE):
        batch = target_sentences[i : i + BATCH_SIZE]
        try:
            encoded = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)
            with torch.no_grad():
                outputs = model(**encoded, output_hidden_states=True)
                # [FIX]: Axis was calculated using only the last layer.
                # To align with the axis space, we must use the last hidden state directly.
                hs = outputs.last_hidden_state
            
            for j, s in enumerate(batch):
                start = s.find(target_word)
                if start == -1: continue
                # ç®€å•æ˜ å°„ï¼šå‡è®¾å­—ç²’åº¦ï¼Œ[CLS]åç§»1
                valid_len = min(len(target_word), 510)
                if start + 1 + valid_len >= 512: continue
                vec = hs[j, start+1:start+1+valid_len, :].mean(dim=0).cpu().numpy()
                if np.isnan(vec).any():
                    # Skip NaN vectors to prevent polluting the average
                    continue
                vectors.append(vec)
        except Exception as e:
            # print(f"Error: {e}")
            pass
        
    if not vectors: return None
    avg = np.mean(vectors, axis=0)
    norm = np.linalg.norm(avg)
    if norm > 0: avg = avg / norm
    return avg

def load_sikubert(path, device=None):
    if not device: device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"åŠ è½½æ¨¡å‹: {path} ({device})...")
    tokenizer = AutoTokenizer.from_pretrained(path)
    model = AutoModel.from_pretrained(path)
    model.eval()
    model.to(device)
    return tokenizer, model, device

def parse_dictionary(filepath, limit=9999):
    words = []
    filename = os.path.basename(filepath)
    category = CAT_MAP.get(filename, filename.replace(".txt", ""))
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line: continue
            # é‡åˆ°åˆ†éš”çº¿åœæ­¢è¯»å–
            if line.startswith('---'): break
            
            parts = line.split(',')
            if len(parts) < 2: continue
            w = parts[0].strip()
            tag = parts[1].strip().lower()
            pol = ""
            if tag in ['pos', 'positive']: pol = "pos"
            elif tag in ['neg', 'negative', 'nag']: pol = "neg"
            
            if pol:
                words.append({"word": w, "category": category, "polarity": pol})
    return words

def main():
    # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
    random.seed(42)
    np.random.seed(42)
    
    parser = argparse.ArgumentParser(description="å†æ—¶ MVP è®¡ç®— (å…¨é‡ç‰ˆ)")
    parser.add_argument('--dic-dir', type=str, help='è¯å…¸ç›®å½•')
    parser.add_argument('--data-dir', type=str, help='å¤šä¹‰è¯æ•°æ®ç›®å½•')
    parser.add_argument('--corpus-dir', type=str, help='è¯­æ–™åº“ç›®å½•')
    parser.add_argument('--axis-file', type=str, help='æ€§åˆ«è½´çº¿æ–‡ä»¶')
    parser.add_argument('--output-dir', type=str, help='ç»“æœç›®å½•')
    parser.add_argument('--model-path', type=str, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--sample-size', type=int, default=DEFAULT_SAMPLE_SIZE, help='æŠ½æ ·æ•°é‡')
    
    # ç¯å¢ƒæ£€æµ‹
    # ä¼˜å…ˆæ£€æµ‹ AutoDL (å³ä½¿åœ¨ Jupyter ä¸­è¿è¡Œä¹Ÿèƒ½æ­£ç¡®è¯†åˆ«)
    if os.path.exists("/root/autodl-tmp"):
        print("æ£€æµ‹åˆ° AutoDL ç¯å¢ƒ...")
        args = parser.parse_args([]) # Parse args for AutoDL environment
        # å®šä¹‰ AutoDL å¸¸ç”¨è·¯å¾„
        AUTODL_ROOT = Path("/root/autodl-tmp")
        
        # 1. è¯å…¸ (å‡è®¾åœ¨é¡¹ç›®å†…)
        if args.dic_dir is None:
            # å°è¯•æ‰¾ /root/autodl-tmp/moral_bias/static/dic
            possible_dic = AUTODL_ROOT / "dic"
            if possible_dic.exists(): args.dic_dir = str(possible_dic)
            else: args.dic_dir = "static/dic" # é»˜è®¤ç›¸å¯¹è·¯å¾„
            
        # 2. å¤šä¹‰è¯æ•°æ® (ngram_match_sampled)
        if args.data_dir is None:
            cands = [
                AUTODL_ROOT / "ngram_match_sampled",
                AUTODL_ROOT / "moral_bias/data/ngram_match_sampled",
                Path("/root/ngram_match_sampled")
            ]
            for p in cands:
                if p.exists():
                    args.data_dir = str(p)
                    print(f"  è‡ªåŠ¨åŒ¹é… Data Dir: {p}")
                    break
        
        # 3. æ ¸å¿ƒå¤ç±
        if args.corpus_dir is None:
            cands = [
                AUTODL_ROOT / "æ ¸å¿ƒå¤ç±",
                AUTODL_ROOT / "data/æ ¸å¿ƒå¤ç±",
                AUTODL_ROOT / "moral_bias/data/æ ¸å¿ƒå¤ç±"
            ]
            for p in cands:
                if p.exists():
                    args.corpus_dir = str(p)
                    print(f"  è‡ªåŠ¨åŒ¹é… Corpus Dir: {p}")
                    break
                    
        if args.axis_file is None:
            cands = [
                # æ ¹æ®æˆªå›¾è°ƒæ•´ï¼šç›´æ¥åœ¨ autodl-tmp ä¸‹
                AUTODL_ROOT / "20251230-diachronic-axis/dynasty_gender_axes.npy",
                AUTODL_ROOT / "result/20251230-diachronic-axis/dynasty_gender_axes.npy",
                AUTODL_ROOT / "moral_bias/result/20251230-diachronic-axis/dynasty_gender_axes.npy"
            ]
            for p in cands:
                if p.exists():
                    args.axis_file = str(p)
                    print(f"  è‡ªåŠ¨åŒ¹é… Axis File: {p}")
                    break
                    
        # 5. æ¨¡å‹è·¯å¾„
        if args.model_path is None:
            cands = [
                AUTODL_ROOT / "model/sikubert",
                AUTODL_ROOT / "sikubert",
                AUTODL_ROOT / "moral_bias/model/sikubert"
            ]
            for p in cands:
                if p.exists():
                    args.model_path = str(p)
                    print(f"  è‡ªåŠ¨åŒ¹é…æœ¬åœ°æ¨¡å‹: {p}")
                    break
                    
        # 6. è¾“å‡º
        if args.output_dir is None:
            args.output_dir = str(AUTODL_ROOT / "result/20251230-moral-mvp-diachronic")

    elif 'ipykernel' in sys.modules or 'google.colab' in sys.modules:
        print("æ£€æµ‹åˆ° Jupyter/Colab (é AutoDL) ç¯å¢ƒ...")
        args = parser.parse_args([])
        if args.dic_dir is None and COLAB_DIC_DIR.exists(): args.dic_dir = str(COLAB_DIC_DIR)
        if args.data_dir is None and COLAB_NGRAM_MATCH_DIR.exists(): args.data_dir = str(COLAB_NGRAM_MATCH_DIR)
        if args.corpus_dir is None and COLAB_CORPUS_DIR.exists(): args.corpus_dir = str(COLAB_CORPUS_DIR)
        if args.axis_file is None and COLAB_AXIS_FILE.exists(): args.axis_file = str(COLAB_AXIS_FILE)
        if args.output_dir is None: args.output_dir = str(COLAB_OUTPUT_DIR)
        
    else:
        args = parser.parse_args()

    # å‚æ•°å›é€€
    dic_dir = args.dic_dir if args.dic_dir else DEFAULT_DIC_DIR
    data_dir = args.data_dir if args.data_dir else DEFAULT_NGRAM_MATCH_DIR
    corpus_dir = args.corpus_dir if args.corpus_dir else DEFAULT_CORPUS_DIR
    model_path = args.model_path if args.model_path else DEFAULT_MODEL_NAME
    axis_file = args.axis_file if args.axis_file else DEFAULT_AXIS_FILE
    output_dir = args.output_dir if args.output_dir else DEFAULT_OUTPUT_DIR
    sample_size = args.sample_size
    
    os.makedirs(output_dir, exist_ok=True)
    print("=" * 60)
    print("ğŸš€ å¼€å§‹å†æ—¶ MVP è®¡ç®—ä»»åŠ¡ (Diachronic)")
    print("=" * 60)
    print(f"é…ç½®:\n Dic: {dic_dir}\n Data: {data_dir}\n Corpus: {corpus_dir}\n Axis: {axis_file}")
    
    # 1. åŠ è½½
    print(f"\n[Step 1/5] åŠ è½½æ¨¡å‹ä¸èµ„æº...")
    tokenizer, model, device = load_sikubert(model_path)
    if not os.path.exists(axis_file):
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°æ€§åˆ«è½´çº¿æ–‡ä»¶ {axis_file}")
        return
    axes_data = np.load(axis_file, allow_pickle=True).item()
    print(f"  -> æ€§åˆ«è½´çº¿åŠ è½½å®Œæˆï¼ŒåŒ…å«æœä»£: {list(axes_data.keys())}")
    
    # 2. æ”¶é›†è¯
    print(f"\n[Step 2/5] é¢„æ‰«æè¯å…¸...")
    all_target_info = []
    dict_files = glob.glob(os.path.join(dic_dir, "*.txt"))
    for df in dict_files:
        if "dic_filter" in df: continue
        all_target_info.extend(parse_dictionary(df, limit=9999))
    
    target_words = list(set([x['word'] for x in all_target_info]))
    word_info_map = {x['word']: x for x in all_target_info}
    print(f"  -> æå–ç›®æ ‡è¯: {len(target_words)} ä¸ª")
    
    chronological_scores = {}
    
    # 3. å†æ—¶è®¡ç®—
    # é¢„å…ˆåŠ è½½æ‰€æœ‰è¯å…¸å†…å®¹ä»¥åŠ é€Ÿå¤šä¹‰è¯åˆ¤æ–­
    print(f"  -> ç¼“å­˜è¯å…¸å†…å®¹ä»¥åŠ é€ŸæŸ¥æ‰¾...")
    dict_content_cache = {}
    for df in dict_files:
        dict_content_cache[df] = open(df, encoding='utf-8').read()
        
    dynasty_idx = 0
    total_dynasties = len([d for d in DYNASTY_ORDER if DYNASTY_MAP.get(d)])

    for dynasty in DYNASTY_ORDER:
        d_key = DYNASTY_MAP.get(dynasty)
        if not d_key: continue
        
        dynasty_idx += 1
        print(f"\n" + "-" * 50)
        print(f"[Step 3/5] å¤„ç†æœä»£ ({dynasty_idx}/{total_dynasties}): {dynasty} ({d_key})")
        print("-" * 50)
        
        axis = axes_data.get(dynasty)
        axis = axis / np.linalg.norm(axis)
        
        # å‡†å¤‡æ•°æ®
        sentences_map = {}
        # é¢„å…ˆåˆ¤æ–­å“ªäº›æ˜¯å¤šä¹‰è¯
        poly_words = []
        mono_words = []
        poly_load_success = 0
        
        print(f"  -> 3.1 åŒºåˆ†å•/å¤šä¹‰è¯å¹¶åŠ è½½å¤šä¹‰è¯ä¾‹å¥...")
        # ä½¿ç”¨ progress bar
        for w in tqdm(target_words, desc="    è¯æ±‡åˆ†ç±»", unit="word"):
            # ç®€å•åˆ¤æ–­ï¼Œè¿™é‡Œéœ€è¦éå†æ‰€æœ‰å­—å…¸æ–‡ä»¶ç¡®è®¤æ˜¯å¦polyï¼Œç•¥æ…¢
            # ä¼˜åŒ–ï¼šåœ¨parse_dictionaryæ—¶å°±è®°å½•æ˜¯å¦poly
            # è¿™é‡Œå¤ç”¨åŸæœ‰é€»è¾‘ï¼šå®æ—¶æŸ¥æ‰¾
            is_poly = False
            found_df = None
            category_code = "00"
            polarity = word_info_map[w]['polarity']
            
            for df in dict_files:
                if w in dict_content_cache[df]: # ä½¿ç”¨ç¼“å­˜åŠ é€Ÿ
                     if is_polysemous_word(w, df):
                        is_poly = True
                        found_df = df
                        fname = os.path.basename(df)
                        category_code = fname[:2] if fname[0].isdigit() else "00"
                        break
            
            if is_poly:
                poly_words.append(w)
                sents = load_filtered_sentences_for_polysemy(w, category_code, polarity, data_dir, d_key, sample_size)
                sentences_map[w] = sents
                if sents:
                    poly_load_success += 1
            else:
                mono_words.append(w)
                sentences_map[w] = []
        
        # æ˜¾ç¤ºå¤šä¹‰è¯ç»Ÿè®¡
        if poly_words:
            print(f"     å¤šä¹‰è¯: {len(poly_words)}ä¸ª, æˆåŠŸåŠ è½½: {poly_load_success}/{len(poly_words)}")
        
        # æ‰¹é‡é‡‡é›†å•ä¹‰è¯
        if mono_words:
            print(f"  -> 3.2 è¯­æ–™åº“æ‰«æ ({len(mono_words)} ä¸ªå•ä¹‰è¯)...")
            bulk = extract_sentences_from_corpus(corpus_dir, d_key, mono_words, sample_size)
            mono_success = sum(1 for w in mono_words if bulk.get(w))
            for w, s in bulk.items():
                sentences_map[w] = s
            print(f"     å•ä¹‰è¯ç»Ÿè®¡: {len(mono_words)}ä¸ª, æˆåŠŸé‡‡é›†: {mono_success}/{len(mono_words)}")
            print(f"     é‡‡é›†å®Œæˆ")
        
        # ä¸´æ—¶å­˜å‚¨è¯¥æœä»£çš„æ‰€æœ‰å‘é‡
        # list of (word, vector)
        epoch_vectors = []
        
        # è®¡ç®—
        print(f"  -> 3.3 è®¡ç®—è¯å‘é‡...")
        pbar = tqdm(target_words, desc="    æå–å‘é‡", unit="word")
        for w in pbar:
            sents = sentences_map.get(w, [])
            if not sents:
                # è¯¦ç»†æŠ¥é”™é€»è¾‘ (çœç•¥ä»¥ç®€åŒ–è¾“å‡ºï¼Œä¿ç•™æ ¸å¿ƒæç¤º)
                # print(f"\033[91m  âŒ CRITICAL ERROR: è¯æ±‡ '{w}' åœ¨æœä»£ '{dynasty}' ({d_key}) æœªæ‰¾åˆ°ä»»ä½•ä¾‹å¥ï¼\033[0m")
                continue 
            
            vec = get_contextual_vector(model, tokenizer, device, sents, w)
            if vec is not None:
                # æ£€æŸ¥ NaN
                if np.isnan(vec).any():
                    pbar.write(f"     [è­¦å‘Š] {w} å‘é‡åŒ…å« NaNï¼Œè·³è¿‡")
                    continue
                    
                epoch_vectors.append((w, vec))
            else:
                 # pbar.write(f"\033[91m  âŒ CRITICAL ERROR: è¯æ±‡ '{w}' åœ¨æœä»£ '{dynasty}' è®¡ç®—å‘é‡å¤±è´¥ï¼ˆè¿”å› Noneï¼‰ï¼\033[0m")
                 pass

        # ç»Ÿä¸€å»å‡å€¼å¹¶è®¡ç®—è¿™ä¸ªæœä»£çš„åˆ†æ•°
        if epoch_vectors:
            # 1. è®¡ç®—å½“å‰æœä»£çš„å…¨å±€å‡å€¼
            all_vecs = np.stack([v for _, v in epoch_vectors])
            epoch_mean = np.mean(all_vecs, axis=0)
            
            print(f"  âš¡ [æœä»£: {dynasty}] å·²åº”ç”¨å»å‡å€¼ (Common Mean Removal)")
            
            # 2. å»å‡å€¼å¹¶æŠ•å½±
            for w, vec in epoch_vectors:
                # å»å‡å€¼
                vec_centered = vec - epoch_mean
                
                # å½’ä¸€åŒ–
                norm = np.linalg.norm(vec_centered)
                if norm > 0:
                    vec_centered = vec_centered / norm
                
                # æŠ•å½±
                score = float(np.dot(vec_centered, axis))
                
                if w not in chronological_scores: chronological_scores[w] = {}
                chronological_scores[w][dynasty] = score
        else:
            print(f"  âš ï¸  æœä»£ {dynasty} æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆå‘é‡")
                
    # 4. è¾“å‡º
    print(f"\n[Step 4/5] æ•´åˆå¹¶è¾“å‡ºç»“æœ...")
    output_data = []
    for w, scores in chronological_scores.items():
        info = word_info_map.get(w, {})
        output_data.append({
            "word": w, "category": info.get("category"), "polarity": info.get("polarity"),
            "scores": scores
        })
        
    json_path = os.path.join(output_dir, "moral_bias_mvp_diachronic_full.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
        
    print(f"âœ… [Done] JSON å·²ä¿å­˜: {json_path}") 
    print(f"ğŸ‰ å…¨éƒ¨ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    main()
