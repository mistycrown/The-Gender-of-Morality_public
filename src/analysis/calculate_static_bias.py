"""
================================================================================
é“å¾·è¯å…¸æ€§åˆ«åè§ MVPè®¡ç®—è„šæœ¬ (é™æ€ç‰ˆ - å…¨é‡å®éªŒ)
================================================================================
æ¯æ¬¡æ›´æ–°ä»£ç æ–‡ä»¶æ—¶è¯·æ›´æ–°ä»£ç å¤´æ³¨é‡Šï¼

ã€åŠŸèƒ½æè¿°ã€‘
1. è¯»å– static/dic/ ä¸‹çš„é“å¾·è¯å…¸æ–‡ä»¶ï¼ˆå…¨é‡è¯»å–ï¼Œä¸é™åˆ¶æ•°é‡ï¼‰ã€‚
2. ä½¿ç”¨ SikuBERT æ¨¡å‹æå–è¯å‘é‡ï¼š
   - å•ä¹‰è¯ï¼šä»è¯­æ–™åº“éšæœºæŠ½æ ·200æ¡ä¾‹å¥ï¼ŒæŒ‰æœä»£åˆ†å±‚ï¼Œæå–ä¸Šä¸‹æ–‡å‘é‡
   - å¤šä¹‰è¯ï¼šä»ç­›é€‰ä¾‹å¥ä¸­æŠ½æ ·200æ¡ï¼ŒæŒ‰æœä»£åˆ†å±‚ï¼Œæå–ä¸Šä¸‹æ–‡å‘é‡
   - æ‰€æœ‰è¯æ±‡ç»Ÿä¸€ä½¿ç”¨ä¸Šä¸‹æ–‡æ•æ„Ÿå‘é‡ï¼ˆBERT last hidden state å¹³å‡ï¼‰
3. è®¡ç®—è¿™äº›è¯åœ¨ SikuBERT é™æ€æ€§åˆ«è½´çº¿ä¸Šçš„æŠ•å½±å¾—åˆ† (Bias Score)ã€‚
4. è¾“å‡º Markdown æ ¼å¼çš„ç»Ÿè®¡è¡¨æ ¼å’Œ JSON æ•°æ®ã€‚

ã€è¾“å…¥ã€‘
- static/dic/*.txt: é“å¾·è¯å…¸æ–‡ä»¶
- data/ngram_match_sampled/*.txt: å¤šä¹‰è¯ç­›é€‰ä¾‹å¥æ–‡ä»¶
- data/æ ¸å¿ƒå¤ç±/*.txt: æ ¸å¿ƒå¤ç±è¯­æ–™åº“ï¼ˆæŒ‰æœä»£åˆ†æ–‡ä»¶ï¼‰
- result/20251230-gender-axis/gender_axis.npy: é™æ€æ€§åˆ«è½´çº¿

ã€è¾“å‡ºã€‘
- result/20251230-moral-mvp-static/moral_bias_mvp_static_full.md
- result/20251230-moral-mvp-static/moral_bias_mvp_static_full.json

ã€å‚æ•°è¯´æ˜ã€‘
- --dic-dir: è¯å…¸ç›®å½•
- --data-dir: ngram_match ç›®å½• (å¤šä¹‰è¯ä¾‹å¥)
- --corpus-dir: æ ¸å¿ƒå¤ç±è¯­æ–™ç›®å½•
- --model-path: æ¨¡å‹è·¯å¾„
- --axis-file: æ€§åˆ«è½´çº¿æ–‡ä»¶ (.npy)
- --output-dir: ç»“æœè¾“å‡ºç›®å½•
- --sample-size: æ¯ä¸ªè¯æŠ½æ ·ä¾‹å¥æ•°é‡ (é»˜è®¤200)

ã€ä¾èµ–åº“ã€‘
- transformers, torch, numpy, tqdm

ã€ä½œè€…ã€‘AIè¾…åŠ©ç”Ÿæˆ
ã€åˆ›å»ºæ—¥æœŸã€‘2026-01-11
ã€æœ€åä¿®æ”¹ã€‘
- 2024-12-30: åˆ›å»ºåˆå§‹ç‰ˆæœ¬ï¼Œæ”¯æŒç®€å•çš„è¯ä¹‰ç­›é€‰
- 2025-01-11: ä¼˜åŒ– & è¿ç§» AutoDL
    1. å¢åŠ  Batch Processing (128) ä¼˜åŒ– 3090 æ€§èƒ½
    2. å¢åŠ  AutoDL ç¯å¢ƒè‡ªåŠ¨è¯†åˆ« (/root/autodl-tmp)
    3. å¢åŠ æœ¬åœ°æ¨¡å‹è‡ªåŠ¨æ£€æµ‹ (ä¼˜å…ˆäº HF ä¸‹è½½)
    4. ä¿®å¤è¯å…¸è¯»å–é€»è¾‘ï¼ˆå¿½ç•¥åˆ†éš”çº¿åå†…å®¹ï¼‰
    5. å¢åŠ å¯åŠ¨æç¤ºï¼Œé¿å…åŠ è½½æ—¶å‡æ­»ä½“éªŒ
    6. ä¿®å¤ IndentationError å’Œå˜é‡åˆå§‹åŒ–é—®é¢˜
- 2026-01-12: é‡æ„è¯å‘é‡æå–é€»è¾‘
    1. ä¿®å¤å¤šä¹‰è¯ä¾‹å¥æ ¼å¼è§£æï¼ˆæ­£ç¡®æå–ç¬¬ä¸‰ä¸ªæ–¹æ‹¬å·çš„ç›¸ä¼¼åº¦å’Œæœä»£æ ‡è®°ï¼‰
    2. å•ä¹‰è¯æ”¹ç”¨ä¸Šä¸‹æ–‡å‘é‡ï¼ˆä»è¯­æ–™åº“æŠ½æ ·200æ¡ä¾‹å¥ï¼‰
    3. æ·»åŠ æœä»£åˆ†å±‚æŠ½æ ·æœºåˆ¶ï¼ˆstratified_sample_by_dynastyï¼‰
    4. ç»Ÿä¸€å•ä¹‰è¯å’Œå¤šä¹‰è¯çš„å‘é‡æå–æµç¨‹ï¼ˆéƒ½ä½¿ç”¨ get_contextual_word_vectorï¼‰
    5. ç§»é™¤é™æ€è¯å‘é‡å‡½æ•°ï¼ˆget_word_vectorï¼‰
    6. æ·»åŠ è¯­æ–™åº“ç›®å½•é…ç½®å’Œè‡ªåŠ¨åŒ¹é…é€»è¾‘

ã€ä¿®æ”¹å†å²ã€‘
- 2026-01-11: åŸºäº calculate_moral_mvp_static.py åˆ›å»ºå…¨é‡ç‰ˆï¼Œå¢åŠ  Colab æ”¯æŒå’Œå…¨é‡æ•°æ®å¤„ç†èƒ½åŠ›
- 2026-01-12: é‡å¤§é‡æ„ï¼Œç»Ÿä¸€è¯å‘é‡æå–é€»è¾‘ï¼Œä¿®å¤å¤šä¹‰è¯è§£æï¼Œå¢åŠ æœä»£åˆ†å±‚æŠ½æ ·
================================================================================
"""

import os

# å¿…é¡»åœ¨ import transformers ä¹‹å‰è®¾ç½®ï¼Œå¦åˆ™å¯èƒ½æ— æ•ˆ
if os.path.exists("/root/autodl-tmp"):
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

print("â³ æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒ (åŠ è½½ PyTorch/Transformers)...") # æç¤ºç”¨æˆ·æ­£åœ¨åŠ è½½åº“

import sys
import glob
import re
import argparse
import random
import json
import numpy as np
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModel
from typing import List, Dict, Tuple
from tqdm import tqdm  # æ·»åŠ è¿›åº¦æ¡åº“

# ==================== é»˜è®¤é…ç½® ====================
DEFAULT_MODEL_NAME = "SIKU-BERT/sikubert"
DEFAULT_DIC_DIR = "static/dic"
DEFAULT_NGRAM_MATCH_DIR = r"data\ngram_match_sampled"
DEFAULT_CORPUS_DIR = r"data\æ ¸å¿ƒå¤ç±"  # æ–°å¢ï¼šè¯­æ–™åº“ç›®å½•
DEFAULT_AXIS_FILE = r"result\20251230-gender-axis\gender_axis.npy"
DEFAULT_OUTPUT_DIR = r"result\20251230-moral-mvp-static"
DEFAULT_SAMPLE_SIZE = 200  # æ–°å¢ï¼šæ¯ä¸ªè¯æŠ½æ ·ä¾‹å¥æ•°é‡
POLYSEMY_THRESHOLD = 0.05  # å¤šä¹‰è¯ç›¸ä¼¼åº¦æˆªæ–­é˜ˆå€¼
BATCH_SIZE = 128  # é’ˆå¯¹ 3090 ä¼˜åŒ–

# colab é»˜è®¤è·¯å¾„
COLAB_ROOT = Path("/content/drive/MyDrive/moral_bias_data")
COLAB_DIC_DIR = COLAB_ROOT / "dic"
COLAB_NGRAM_MATCH_DIR = COLAB_ROOT / "ngram_match"
COLAB_CORPUS_DIR = COLAB_ROOT / "corpus"  # æ–°å¢ï¼šColab è¯­æ–™åº“è·¯å¾„
COLAB_AXIS_FILE = COLAB_ROOT / "gender_axis/gender_axis.npy"
COLAB_OUTPUT_DIR = COLAB_ROOT / "result/20251230-moral-mvp-static"

# æœä»£æ˜ å°„ï¼ˆç”¨äºæœä»£åˆ†å±‚æŠ½æ ·ï¼‰
DYNASTY_MAP = {
    'å…ˆç§¦ä¸¤æ±‰': 'Aå…ˆç§¦ä¸¤æ±‰', 'é­æ™‹å—åŒ—æœ': 'Bé­æ™‹å—åŒ—æœ', 'éš‹å”': 'Céš‹å”',
    'å®‹': 'Då®‹', 'å…ƒ': 'Eå…ƒ', 'æ˜': 'Fæ˜', 'æ¸…': 'Gæ¸…'
}
DYNASTY_ORDER = ['å…ˆç§¦ä¸¤æ±‰', 'é­æ™‹å—åŒ—æœ', 'éš‹å”', 'å®‹', 'å…ƒ', 'æ˜', 'æ¸…']

# è¯å…¸æ–‡ä»¶ååˆ°ç±»åˆ«çš„æ˜ å°„
CAT_MAP = {
    "01care.txt": "Care",
    "02aut.txt": "Authority",
    "03loy.txt": "Loyalty",
    "04fair.txt": "Fairness",
    "05san.txt": "Sanctity",
    "06lenience.txt": "Lenience",
    "07waste.txt": "Waste",
    "08altruism.txt": "Altruism",
    "09diligence.txt": "Diligence",
    "10resilience.txt": "Resilience",
    "11modesty.txt": "Modesty",
    "12valor.txt": "Valor"
}

def stratified_sample_by_dynasty(sentences: List[Tuple[str, str]], target_count: int = 200) -> List[Tuple[str, str]]:
    """æŒ‰æœä»£åˆ†å±‚æŠ½æ ·
    
    Args:
        sentences: [(æœä»£, ä¾‹å¥æ­£æ–‡), ...] å…ƒç»„åˆ—è¡¨
        target_count: ç›®æ ‡æ ·æœ¬æ•°
    
    Returns:
        æŠ½æ ·åçš„ä¾‹å¥åˆ—è¡¨ï¼Œç¡®ä¿å„æœä»£æ¯”ä¾‹å‡è¡¡
    """
    if len(sentences) <= target_count:
        return sentences
    
    # æŒ‰æœä»£åˆ†ç»„
    dynasty_groups = {}
    for dynasty, sent in sentences:
        if dynasty not in dynasty_groups:
            dynasty_groups[dynasty] = []
        dynasty_groups[dynasty].append((dynasty, sent))
    
    # è®¡ç®—æ¯ä¸ªæœä»£åº”æŠ½æ ·çš„æ•°é‡
    num_dynasties = len(dynasty_groups)
    if num_dynasties == 0:
        return []
    
    samples_per_dynasty = target_count // num_dynasties
    remainder = target_count % num_dynasties
    
    # ä»æ¯ä¸ªæœä»£æŠ½æ ·
    result = []
    for i, (dynasty, group) in enumerate(dynasty_groups.items()):
        # å‰ remainder ä¸ªæœä»£å¤šæŠ½1ä¸ª
        n_samples = samples_per_dynasty + (1 if i < remainder else 0)
        n_samples = min(n_samples, len(group))
        result.extend(random.sample(group, n_samples))
    
    return result

def extract_sentences_from_corpus(corpus_dir: str, target_words: List[str], limit_per_word: int = 200) -> Dict[str, List[Tuple[str, str]]]:
    """ä»è¯­æ–™åº“ä¸­æå–åŒ…å«ç›®æ ‡è¯çš„å¥å­ï¼ˆå…¨æœä»£æ··åˆï¼‰
    
    Args:
        corpus_dir: è¯­æ–™åº“ç›®å½•
        target_words: ç›®æ ‡è¯æ±‡åˆ—è¡¨
        limit_per_word: æ¯ä¸ªè¯é™åˆ¶æŠ½æ ·æ•°é‡
    
    Returns:
        {è¯æ±‡: [(æœä»£, ä¾‹å¥), ...], ...}
    """
    word_sents = {w: [] for w in target_words}
    
    # æ‰«ææ‰€æœ‰è¯­æ–™æ–‡ä»¶
    files = glob.glob(os.path.join(corpus_dir, "*.txt"))
    random.shuffle(files)
    
    print(f"  è¯­æ–™æ–‡ä»¶: {len(files)} ä¸ª")
    
    # ç”¨äºè°ƒè¯•ï¼šè®°å½•æ¯ä¸ªè¯åœ¨æ‰€æœ‰æ–‡ä»¶ä¸­æ‰¾åˆ°çš„æ€»æ•°
    total_found = {w: 0 for w in target_words}
    
    pbar = tqdm(files, desc="  æ‰«æè¯­æ–™", unit="file")
    for fp in pbar:
        # ä»æ–‡ä»¶åæå–æœä»£ä¿¡æ¯
        filename = os.path.basename(fp)
        # å‡è®¾æ–‡ä»¶åæ ¼å¼ï¼šæ ¸å¿ƒå¤ç±-{æœä»£ä»£ç }.txt æˆ–ç±»ä¼¼
        dynasty = "æœªçŸ¥"
        for dyn_name, dyn_code in DYNASTY_MAP.items():
            if dyn_code in filename:
                dynasty = dyn_name
                break
        
        try:
            with open(fp, 'r', encoding='utf-8') as f:
                content = f.read()
                sents = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
                
                for s in sents:
                    s = s.strip()
                    if len(s) < 5 or len(s) > 500:
                        continue
                    
                    for w in target_words:
                        if w in s:
                            total_found[w] += 1  # è®°å½•æ‰¾åˆ°çš„æ€»æ•°
                            if len(word_sents[w]) < limit_per_word:
                                word_sents[w].append((dynasty, s))
        except Exception as e:
            # only print if not a common encoding error to avoid spam, or print first few
            # For now print all to be safe
            print(f"  âŒ Error reading {filename}: {e}")
    
    # è¯¦ç»†ç»Ÿè®¡ç»“æœ
    print(f"\n     è¯­æ–™åº“æ‰«æç»Ÿè®¡:")
    found_counts = {w: len(sents) for w, sents in word_sents.items()}
    missing = [w for w, count in found_counts.items() if count == 0]
    
    # æ‰¾å‡ºé‚£äº›åœ¨è¯­æ–™åº“ä¸­å‡ºç°ä½†å› ä¸ºå·²è¾¾ä¸Šé™è€Œæœªæ”¶å½•çš„è¯
    if missing:
        print(f"     âš ï¸  ä»¥ä¸‹è¯æœªæ‰¾åˆ°ä»»ä½•ä¾‹å¥:")
        for w in missing[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ª
            print(f"        - {w}: åœ¨è¯­æ–™åº“ä¸­å…±å‡ºç°{total_found[w]}æ¬¡ (é•¿åº¦â‰¥5çš„å¥å­)")
    
    # æ‰¾å‡ºæ”¶é›†ä¸è¶³200æ¡çš„è¯
    insufficient = [(w, count) for w, count in found_counts.items() if 0 < count < limit_per_word]
    if insufficient:
        print(f"     â„¹ï¸  ä»¥ä¸‹è¯ä¾‹å¥ä¸è¶³{limit_per_word}æ¡:")
        for w, count in insufficient[:5]:
            print(f"        - {w}: {count}æ¡ (è¯­æ–™åº“ä¸­å…±{total_found[w]}æ¬¡)")
    
    return word_sents

def is_polysemous_word(word: str, dict_filepath: str) -> bool:
    """æ£€æŸ¥è¯æ±‡æ˜¯å¦ä¸ºå¤šä¹‰è¯"""
    try:
        with open(dict_filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('---'): continue
                
                # Check for marker first to be fast
                if 'å¤šä¹‰' in line:
                    parts = line.split(',')
                    # å…³é”®ç‚¹ï¼šå¿…é¡»æ£€æŸ¥è¿™ä¸€è¡Œçš„â€œä¸»è¯â€ï¼ˆé€—å·å‰ç¬¬ä¸€ä¸ªè¯ï¼‰æ˜¯ä¸æ˜¯æˆ‘ä»¬è¦æ‰¾çš„ word
                    # å¦åˆ™ï¼Œå› ä¸ºâ€œä¹‰â€å­—åŒ…å«åœ¨â€œï¼ˆå¤šä¹‰ï¼‰â€æ ‡è®°é‡Œï¼Œå¦‚æœä¸æ£€æŸ¥ä¸»è¯ï¼Œ
                    # ä»»ä½•å¸¦æœ‰â€œï¼ˆå¤šä¹‰ï¼‰â€æ ‡è®°çš„è¡Œéƒ½ä¼šè¢«è¯¯åˆ¤ä¸ºâ€œä¹‰â€å­—çš„å¤šä¹‰è¡Œã€‚
                    if len(parts) >= 1 and parts[0].strip() == word:
                        return True
    except Exception:
        pass
    return False

def load_filtered_sentences_for_polysemy(word: str, category_code: str, label: str, data_dir: str, limit: int = 200) -> List[Tuple[str, str]]:
    """è¯»å–å¤šä¹‰è¯ä¾‹å¥ï¼Œè¿”å› (æœä»£, ä¾‹å¥æ­£æ–‡) åˆ—è¡¨
    
    ä¾‹å¥æ ¼å¼: [Då®‹] [05å­è—] [0.07]åä¸€å¹´æ˜¥å…¬è‡³è‡ªæ™‹æ™‹äººä»¥å…¬ä¸ºè´°äºæ¥šæ•…æ­¢å…¬âœ“
    """
    # å°è¯•å¤šä¸ªæ ‡ç­¾å˜ä½“
    label_variants = [label]
    if label == 'neg':
        label_variants.extend(['nag', 'negative'])
    elif label == 'pos':
        label_variants.extend(['positive'])
    
    matches = []
    for label_variant in label_variants:
        pattern = f"{category_code}-{label_variant}-{word}-*.txt"
        search_path = os.path.join(data_dir, pattern)
        matches = glob.glob(search_path)
        if matches:
            break  # æ‰¾åˆ°å°±åœæ­¢
    
    if not matches:
        return []
        
    filepath = matches[0]
    sentences = []
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                    
                # æå–æ‰€æœ‰æ–¹æ‹¬å·å†…å®¹
                brackets = re.findall(r'\[([^\]]+)\]', line)
                if len(brackets) < 3:
                    continue
                
                # ç¬¬ä¸€ä¸ªæ–¹æ‹¬å·ï¼šæœä»£æ ‡è®° (å¦‚ "Då®‹")
                dynasty_tag = brackets[0]
                
                # ç¬¬ä¸‰ä¸ªæ–¹æ‹¬å·ï¼šç›¸ä¼¼åº¦
                try:
                    similarity = float(brackets[2])
                except:
                    similarity = 0.0
                
                # æå–ä¾‹å¥æ­£æ–‡ï¼šå»é™¤å‰ä¸‰ä¸ªæ–¹æ‹¬å·åŠå…¶å†…å®¹
                content = re.sub(r'^(\[[^\]]+\]\s*){3}', '', line)
                content = content.replace('âœ“', '').replace('âœ—', '').strip()
                
                is_marked_positive = 'âœ“' in line
                is_marked_negative = 'âœ—' in line

                # è¿‡æ»¤é€»è¾‘
                if is_marked_negative:
                    continue
                if not is_marked_positive and similarity <= POLYSEMY_THRESHOLD:
                    continue
                
                if len(content) >= 5 and len(content) <= 500:
                    # æå–æœä»£åç§°ï¼ˆå»æ‰å‰ç¼€å­—æ¯ï¼Œå¦‚ "Då®‹" -> "å®‹"ï¼‰
                    dynasty = dynasty_tag[1:] if len(dynasty_tag) > 1 else dynasty_tag
                    sentences.append((dynasty, content))
                        
    except Exception:
        pass
        
    if len(sentences) > limit:
        sentences = random.sample(sentences, limit)
        
    return sentences

def get_contextual_word_vector(word: str, sentences: List[Tuple[str, str]], tokenizer, model, device) -> np.ndarray:
    """è·å–ä¸Šä¸‹æ–‡å‘é‡ (æ‰¹é‡å¤„ç†ä¼˜åŒ–ç‰ˆ)
    
    Args:
        word: ç›®æ ‡è¯æ±‡
        sentences: [(æœä»£, ä¾‹å¥æ­£æ–‡), ...] å…ƒç»„åˆ—è¡¨
        tokenizer: åˆ†è¯å™¨
        model: BERTæ¨¡å‹
        device: è®¾å¤‡
    """
    vectors = []
    # æå–ä¾‹å¥æ­£æ–‡éƒ¨åˆ†
    target_sentences = [sent for _, sent in sentences[:200]]  # å–å‰200ä¸ªä¾‹å¥
    
    for i in range(0, len(target_sentences), BATCH_SIZE):
        batch = target_sentences[i : i + BATCH_SIZE]
        if not batch: continue
        
        try:
            # æ‰¹é‡ Tokenize
            inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model(**inputs, output_hidden_states=True)
                # [FIX]: Use last hidden state to align with axis definition
                hs = outputs.last_hidden_state
            
            # åœ¨ Batch å†…éƒ¨æå–ç›®æ ‡è¯å‘é‡
            for j, sent in enumerate(batch):
                # æ³¨æ„ï¼štokenize å¯èƒ½ä¼šå¯¼è‡´ç®€å•çš„ find ç´¢å¼•ä¸å¯¹é½ï¼Œè¿™é‡Œåšç®€åŒ–å‡è®¾
                # SikuBERT æ˜¯ Character-basedï¼Œæ‰€ä»¥ find çš„ç´¢å¼• + 1 (CLS) é€šå¸¸æ˜¯å‡†çš„
                start_idx = sent.find(word)
                if start_idx == -1: continue
                
                # æ£€æŸ¥ç´¢å¼•æ˜¯å¦è¶Šç•Œ (å› ä¸º truncation)
                input_len = (inputs['input_ids'][j] != tokenizer.pad_token_id).sum().item()
                
                token_start = start_idx + 1
                token_end = token_start + len(word)
                
                if token_end < input_len and token_end < 512:
                    word_vec = hs[j, token_start:token_end, :].mean(dim=0).cpu().numpy()
                    vectors.append(word_vec)

        except Exception as e:
            # print(f"Batch error: {e}")
            continue
    
    if not vectors:
        return None
    avg_vec = np.mean(vectors, axis=0)
    norm = np.linalg.norm(avg_vec)
    if norm > 0:
        avg_vec = avg_vec / norm
    return avg_vec

def load_sikubert(model_name_or_path: str, device: str = None):
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"åŠ è½½æ¨¡å‹: {model_name_or_path} ({device})...")
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
    model = AutoModel.from_pretrained(model_name_or_path)
    model.eval()
    model.to(device)
    model.eval()
    model.to(device)
    return tokenizer, model, device

def parse_dictionary(filepath: str, limit: int = 9999) -> Dict[str, List[str]]:
    """è§£æå­—å…¸ (limit é»˜è®¤ä¸ºå¾ˆå¤§ï¼Œå³å…¨é‡)"""
    pos_words = []
    neg_words = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line: continue
            # é‡åˆ°åˆ†éš”çº¿åœæ­¢è¯»å–
            if line.startswith('---'): break
            
            parts = line.split(',')
            if len(parts) < 2: continue
            word = parts[0].strip()
            tag = parts[1].strip().lower()
            if tag in ['pos', 'positive']:
                if len(pos_words) < limit: pos_words.append(word)
            elif tag in ['neg', 'negative', 'nag']:
                if len(neg_words) < limit: neg_words.append(word)
    return {'pos': pos_words, 'neg': neg_words}

def main():
    # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
    random.seed(42)
    np.random.seed(42)
    
    parser = argparse.ArgumentParser(description="é™æ€ MVP è®¡ç®— (å…¨é‡ç‰ˆ)")
    parser.add_argument('--dic-dir', type=str, help='è¯å…¸ç›®å½•')
    parser.add_argument('--data-dir', type=str, help='ngram_match æ•°æ®ç›®å½•')
    parser.add_argument('--corpus-dir', type=str, help='æ ¸å¿ƒå¤ç±è¯­æ–™ç›®å½•')
    parser.add_argument('--model-path', type=str, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--axis-file', type=str, help='æ€§åˆ«è½´çº¿æ–‡ä»¶')
    parser.add_argument('--output-dir', type=str, help='ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--sample-size', type=int, default=DEFAULT_SAMPLE_SIZE, help='æ¯ä¸ªè¯æŠ½æ ·ä¾‹å¥æ•°é‡')
    
    # ç¯å¢ƒæ£€æµ‹
    # ä¼˜å…ˆæ£€æµ‹ AutoDL (å³ä½¿åœ¨ Jupyter ä¸­è¿è¡Œä¹Ÿèƒ½æ­£ç¡®è¯†åˆ«)
    if os.path.exists("/root/autodl-tmp"):
        print("æ£€æµ‹åˆ° AutoDL ç¯å¢ƒ...")
        # ä¸ºäº†å…¼å®¹ Notebook å’Œ å‘½ä»¤è¡Œï¼Œç®€å•åˆ¤æ–­
        if 'ipykernel' in sys.modules:
            args = parser.parse_args([])
        else:
            args = parser.parse_args()
            
        AUTODL_ROOT = Path("/root/autodl-tmp")
        
        # 1. è¯å…¸
        if args.dic_dir is None:
            possible_dic = AUTODL_ROOT / "dic"
            if possible_dic.exists(): args.dic_dir = str(possible_dic)
            else: args.dic_dir = "static/dic"
            
        # 2. å¤šä¹‰è¯æ•°æ®
        if args.data_dir is None:
            cands = [AUTODL_ROOT / "ngram_match_sampled", AUTODL_ROOT / "moral_bias/data/ngram_match_sampled"]
            for p in cands:
                if p.exists():
                    args.data_dir = str(p)
                    print(f"  è‡ªåŠ¨åŒ¹é… Data Dir: {p}")
                    break
        
        # 3. æ ¸å¿ƒå¤ç±è¯­æ–™åº“
        if args.corpus_dir is None:
            cands = [
                AUTODL_ROOT / "æ ¸å¿ƒå¤ç±",
                AUTODL_ROOT / "data/æ ¸å¿ƒå¤ç±",
                AUTODL_ROOT / "moral_bias/data/æ ¸å¿ƒå¤ç±"
            ]
            for p in cands:
                if p.exists():
                    args.corpus_dir = str(p)
                    print(f"  è‡ªåŠ¨åŒ¹é… Corpus Dir: {p}")
                    break
                    
        # 4. é™æ€è½´çº¿
        if args.axis_file is None:
            cands = [
                AUTODL_ROOT / "result/20251230-gender-axis/gender_axis.npy",
                AUTODL_ROOT / "moral_bias/result/20251230-gender-axis/gender_axis.npy"
            ]
            for p in cands:
                if p.exists():
                    args.axis_file = str(p)
                    print(f"  è‡ªåŠ¨åŒ¹é… Axis File: {p}")
                    break
        
        # 5. æ¨¡å‹è·¯å¾„
        if args.model_path is None:
            cands = [
                AUTODL_ROOT / "model/sikubert",
                AUTODL_ROOT / "sikubert",
                AUTODL_ROOT / "moral_bias/model/sikubert"
            ]
            for p in cands:
                if p.exists():
                    args.model_path = str(p)
                    print(f"  è‡ªåŠ¨åŒ¹é…æœ¬åœ°æ¨¡å‹: {p}")
                    break
        
        # 6. è¾“å‡º
        if args.output_dir is None:
            args.output_dir = str(AUTODL_ROOT / "result/20251230-moral-mvp-static")

    elif 'ipykernel' in sys.modules or 'google.colab' in sys.modules:
        print("æ£€æµ‹åˆ° Jupyter/Colab (é AutoDL) ç¯å¢ƒ...")
        args = parser.parse_args([])
        if args.dic_dir is None and COLAB_DIC_DIR.exists(): args.dic_dir = str(COLAB_DIC_DIR)
        if args.data_dir is None and COLAB_NGRAM_MATCH_DIR.exists(): args.data_dir = str(COLAB_NGRAM_MATCH_DIR)
        if args.corpus_dir is None and COLAB_CORPUS_DIR.exists(): args.corpus_dir = str(COLAB_CORPUS_DIR)
        if args.axis_file is None and COLAB_AXIS_FILE.exists(): args.axis_file = str(COLAB_AXIS_FILE)
        if args.output_dir is None: args.output_dir = str(COLAB_OUTPUT_DIR)
        
    else:
        args = parser.parse_args()

        
    # è·¯å¾„å›é€€åˆ°é»˜è®¤
    dic_dir = args.dic_dir if args.dic_dir else DEFAULT_DIC_DIR
    data_dir = args.data_dir if args.data_dir else DEFAULT_NGRAM_MATCH_DIR
    corpus_dir = args.corpus_dir if args.corpus_dir else DEFAULT_CORPUS_DIR
    model_path = args.model_path if args.model_path else DEFAULT_MODEL_NAME
    axis_file = args.axis_file if args.axis_file else DEFAULT_AXIS_FILE
    output_dir = args.output_dir if args.output_dir else DEFAULT_OUTPUT_DIR
    sample_size = args.sample_size
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"æ•°æ®é…ç½®:\n Dic: {dic_dir}\n Data: {data_dir}\n Corpus: {corpus_dir}\n Model: {model_path}\n Axis: {axis_file}\n Out: {output_dir}\n Sample: {sample_size}")

    print("=" * 60)
    print("ğŸš€ å¼€å§‹é™æ€ MVP è®¡ç®—ä»»åŠ¡")
    print("=" * 60)

    # 1. åŠ è½½èµ„æº
    print(f"\n[Step 1/3] æ­£åœ¨åŠ è½½åŸºç¡€èµ„æº...")
    if not os.path.exists(axis_file):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ€§åˆ«è½´çº¿æ–‡ä»¶ {axis_file}")
        return
    
    print(f"  -> åŠ è½½æ€§åˆ«è½´çº¿: {axis_file}")
    gender_axis = np.load(axis_file)
    gender_axis = gender_axis / np.linalg.norm(gender_axis)
    
    print(f"  -> åŠ è½½ SikuBERT æ¨¡å‹: {model_path}")
    tokenizer, model, device = load_sikubert(model_path)
    print(f"  -> æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}")
    
    # 2. å¤„ç†å­—å…¸
    print(f"\n[Step 2/3] å¼€å§‹å¤„ç†é“å¾·è¯å…¸...")
    dict_files = glob.glob(os.path.join(dic_dir, "*.txt"))
    results = []
    
    total_files = len([f for f in dict_files if not (f.endswith(".bak") or "dic_filter" in f)])
    print(f"  -> æ‰¾åˆ° {total_files} ä¸ªè¯å…¸æ–‡ä»¶å¾…å¤„ç†")
    
    file_idx = 0
    for filepath in dict_files:
        filename = os.path.basename(filepath)
        if filename.endswith(".bak") or filename == "dic_filter.xlsx": continue
        
        file_idx += 1
        category = CAT_MAP.get(filename, filename.replace(".txt", ""))
        
        print(f"\n  ğŸ“ å¤„ç†æ–‡ä»¶ ({file_idx}/{total_files}): {filename} | ç±»åˆ«: {category}")
        
        # å…¨é‡è¯»å–
        words_dict = parse_dictionary(filepath, limit=9999)
        
        # æå–ç¼–å·
        if filename[0].isdigit(): category_code = filename[:2]
        else: category_code = filename.split('.')[0][:2] if len(filename.split('.')[0]) >= 2 else "00"
        
        all_words = []
        for polarity, words in words_dict.items():
            for w in words:
                all_words.append((w, polarity))
                
        print(f"     å…± {len(all_words)} ä¸ªè¯ (Pos: {len(words_dict['pos'])}, Neg: {len(words_dict['neg'])})")
        
        # é¢„å…ˆåŒºåˆ†å•ä¹‰è¯å’Œå¤šä¹‰è¯
        poly_words = []
        mono_words = []
        
        print(f"     - åŒºåˆ†å•/å¤šä¹‰è¯...")
        for word, polarity in all_words:
            if is_polysemous_word(word, filepath):
                poly_words.append((word, polarity))
            else:
                mono_words.append((word, polarity))
        
        print(f"     - å•ä¹‰è¯: {len(mono_words)} ä¸ª, å¤šä¹‰è¯: {len(poly_words)} ä¸ª")
        
        # ä¸ºå•ä¹‰è¯æ‰¹é‡æŠ½å–è¯­æ–™
        sentences_map = {}
        if mono_words:
            print(f"     - ä»è¯­æ–™åº“æ‰¹é‡æŠ½å–å•ä¹‰è¯ä¾‹å¥...")
            mono_word_list = [w for w, _ in mono_words]
            bulk_sents = extract_sentences_from_corpus(corpus_dir, mono_word_list, sample_size)
            
            # æœä»£åˆ†å±‚æŠ½æ ·å¹¶æ·»åŠ è°ƒè¯•ä¿¡æ¯
            for w, sents in bulk_sents.items():
                before_count = len(sents)
                sampled_sents = stratified_sample_by_dynasty(sents, sample_size)
                after_count = len(sampled_sents)
                sentences_map[w] = sampled_sents
                
                # å¦‚æœæŠ½æ ·åå˜æˆ0ï¼Œæ‰“å°è­¦å‘Š
                if before_count > 0 and after_count == 0:
                    print(f"       âš ï¸  {w}: åŸæœ‰{before_count}æ¡ä¾‹å¥ï¼Œåˆ†å±‚æŠ½æ ·åå˜ä¸º0")
        
        # ä¸ºå¤šä¹‰è¯é€ä¸ªæŠ½å–ç­›é€‰ä¾‹å¥
        if poly_words:
            print(f"     - åŠ è½½å¤šä¹‰è¯ç­›é€‰ä¾‹å¥...")
            poly_load_success = 0
            poly_load_fail = []
            
            for word, polarity in tqdm(poly_words, desc="       åŠ è½½ä¾‹å¥", unit="word"):
                sents = load_filtered_sentences_for_polysemy(word, category_code, polarity, data_dir, sample_size)
                
                if sents:
                    # æœä»£åˆ†å±‚æŠ½æ ·
                    sampled = stratified_sample_by_dynasty(sents, sample_size)
                    sentences_map[word] = sampled
                    poly_load_success += 1
                else:
                    sentences_map[word] = []
                    poly_load_fail.append(word)
            
            print(f"       å¤šä¹‰è¯åŠ è½½ç»Ÿè®¡: æˆåŠŸ{poly_load_success}/{len(poly_words)}ä¸ª")
            if poly_load_fail:
                print(f"       âš ï¸  ä»¥ä¸‹å¤šä¹‰è¯æœªæ‰¾åˆ°ç­›é€‰ä¾‹å¥: {poly_load_fail[:10]}")
        
        # 3. ç»Ÿä¸€è®¡ç®—æ‰€æœ‰è¯çš„ä¸Šä¸‹æ–‡å‘é‡ä¸æŠ•å½±
        print(f"     - è®¡ç®—è¯å‘é‡...")
        
        # ä¸´æ—¶å­˜å‚¨è¯¥æ–‡ä»¶æ‰€æœ‰æå–åˆ°çš„å‘é‡ï¼Œç”¨äºå»å‡å€¼
        file_vectors = []  # list of (word, polarity, vector, is_poly)
        
        pbar = tqdm(all_words, desc="       æå–å‘é‡", unit="word")
        for word, polarity in pbar:
            try:
                is_poly = word in [w for w, _ in poly_words]
                sents = sentences_map.get(word, [])
                
                if not sents:
                    pbar.write(f"       [è·³è¿‡] {word}: æ— æœ‰æ•ˆä¾‹å¥")
                    continue
                
                # ç»Ÿä¸€ä½¿ç”¨ä¸Šä¸‹æ–‡å‘é‡
                vec = get_contextual_word_vector(word, sents, tokenizer, model, device)
                    
                if vec is not None:
                    file_vectors.append({
                        "word": word,
                        "polarity": polarity,
                        "vector": vec,
                        "is_poly": is_poly
                    })
            except Exception as e:
                print(f"Error processing {word}: {e}")
                continue
        
        # 4. å»å‡å€¼ä¸æŠ•å½±è®¡ç®—
        if file_vectors:
            # æå–è¯¥æ–‡ä»¶ï¼ˆç±»åˆ«ï¼‰ä¸‹æ‰€æœ‰è¯çš„å‘é‡è¿›è¡Œå»å‡å€¼
            # æ³¨æ„ï¼šç†è®ºä¸Šåº”è¯¥ç”¨å…¨è¯­æ–™å‡å€¼ï¼Œä½†åœ¨MVPåˆ†æä¸­ï¼Œ
            # ä½¿ç”¨å½“å‰ç±»åˆ«ï¼ˆæˆ–æ‰€æœ‰å·²å¤„ç†ç±»åˆ«ï¼‰çš„å‡å€¼æ¥æ ¡å‡†â€œè¯¥é¢†åŸŸçš„èƒŒæ™¯â€ä¹Ÿæ˜¯å¸¸ç”¨åšæ³•ã€‚
            # è¿™é‡Œä¸ºäº†æ“ä½œç®€ä¾¿ä¸”æœ‰æ•ˆï¼Œæˆ‘ä»¬åœ¨æ–‡ä»¶çº§åˆ«ï¼ˆCategoryçº§åˆ«ï¼‰åšä¸€æ¬¡æ ¡å‡†ï¼Œ
            # æˆ–è€…ï¼Œæ›´ä¸¥è°¨çš„åšæ³•æ˜¯ï¼šæ”¶é›† å…¨é‡ å‘é‡åå†åšã€‚
            # 
            # è€ƒè™‘åˆ°è„šæœ¬æ˜¯åˆ†æ–‡ä»¶å¤„ç†çš„ï¼Œä¸ºäº†é¿å…å·¨å¤§çš„å†…å­˜å¼€é”€ï¼ˆè™½ç„¶åªæœ‰å‡ åƒä¸ªå‘é‡å…¶å®è¿˜å¥½ï¼‰ï¼Œ
            # ä¸”æˆ‘ä»¬å¸Œæœ› Category å†…éƒ¨çš„ç›¸å¯¹ä½ç½®å‡†ç¡®ã€‚
            # ä½†ç”¨æˆ·å¸Œæœ›çš„æ˜¯â€œå…¨å±€â€çš„ä¸€è‡´æ€§ã€‚
            # 
            # ä¿®æ­£å†³ç­–ï¼šç”±äºæˆ‘ä»¬åœ¨å¤–å±‚å¾ªç¯æ˜¯é€ä¸ªæ–‡ä»¶å¤„ç†å¹¶ append åˆ° resultsï¼Œ
            # è¿™é‡Œåªæ”¶é›†äº†å½“å‰æ–‡ä»¶çš„å‘é‡ã€‚å¦‚æœåªå‡å»å½“å‰æ–‡ä»¶çš„å‡å€¼ï¼Œ
            # å¯èƒ½ä¼šå¯¼è‡´ä¸åŒ Category çš„â€œé›¶ç‚¹â€ä¸ä¸€æ ·ï¼ˆä¾‹å¦‚â€œæƒå¨â€ç±»çš„æ•´ä½“åç§»å’Œâ€œå…³çˆ±â€ç±»ä¸ä¸€æ ·ï¼‰ã€‚
            # è¿™å¯¹äºè·¨ç±»åˆ«æ¯”è¾ƒï¼ˆBoxplotï¼‰æ˜¯å±é™©çš„ã€‚
            # 
            # çº æ­£æ–¹æ¡ˆï¼šä¸åšå±€éƒ¨å»å‡å€¼ã€‚
            # æ”¹ä¸ºï¼šå°†æ‰€æœ‰æ–‡ä»¶çš„ç»“æœåŒ…å«å‘é‡å…ˆå­˜èµ·æ¥ (results_with_vectors)ï¼Œ
            # å¾ªç¯ç»“æŸåç»Ÿä¸€å»å‡å€¼ï¼Œå†è®¡ç®— scoreã€‚
            
            # --- æš‚æ—¶å…ˆå­˜å…¥ results åˆ—è¡¨ï¼ŒåŒ…å« vector ---
            for item in file_vectors:
                results.append({
                    "word": item['word'], 
                    "category": category, 
                    "polarity": item['polarity'],
                    "vector": item['vector'], # æš‚å­˜å‘é‡
                    "is_polysemous": item['is_poly']
                })
    
    # === æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæ¯•ï¼Œç»Ÿä¸€å»å‡å€¼å¹¶è®¡ç®—åˆ†æ•° ===
    print(f"\n[Step 3/3] å…¨å±€å»å‡å€¼ä¸åˆ†æ•°è®¡ç®—...")
    
    if results:
        # stack vectors
        all_vecs = np.stack([r['vector'] for r in results])
        global_mean = np.mean(all_vecs, axis=0)
        
        print(f"  âš¡ å·²åº”ç”¨å»å‡å€¼ (Common Mean Removal) ç­–ç•¥")
        print(f"     å…¨å±€å‘é‡æ•°: {len(results)}")
        print(f"     å‘é‡ç»´åº¦: {all_vecs.shape}")

        # è®¡ç®—å¹¶æ›´æ–°
        valid_count = 0
        for res in results:
            vec_centered = res['vector'] - global_mean
            # å†æ¬¡å½’ä¸€åŒ–? é€šå¸¸å»å‡å€¼åæ–¹å‘æ”¹å˜ï¼Œé•¿åº¦ä¹Ÿå˜ã€‚
            # åœ¨ analyze_category_relationships_split.py ä¸­ï¼Œæˆ‘ä»¬å»å‡å€¼ååšäº†å½’ä¸€åŒ–ã€‚
            # ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼š
            norm = np.linalg.norm(vec_centered)
            if norm > 0:
                vec_centered = vec_centered / norm
            
            score = float(np.dot(vec_centered, gender_axis))
            res['score'] = score
            
            # ç§»é™¤ vector å­—æ®µä»¥å‡å° JSON ä½“ç§¯
            del res['vector'] 
            valid_count += 1
            
        print(f"  âœ… è®¡ç®—å®Œæˆ: {valid_count} ä¸ªè¯")

    # 4. è¾“å‡ºä¿å­˜ (åŸ Step 3)
    print(f"  æ­£åœ¨ä¿å­˜ç»“æœ...")
    results.sort(key=lambda x: x['score'], reverse=True)
    json_path = os.path.join(output_dir, "moral_bias_mvp_static_full.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ä¿å­˜ JSON: {json_path}")

    md_path = os.path.join(output_dir, "moral_bias_mvp_static_full.md")
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write("# å…¨é‡é™æ€ MVP åˆ†æ\n\n")
        f.write(f"| æ’å | è¯æ±‡ | ç±»åˆ« | ææ€§ | åˆ†æ•° |\n|---|---|---|---|---|\n")
        for i, res in enumerate(results):
            f.write(f"| {i+1} | {res['word']} | {res['category']} | {res['polarity']} | {res['score']:.4f} |\n")
    print(f"ä¿å­˜ Markdown: {md_path}")

if __name__ == "__main__":
    main()
